---
title: 'Linear Discriminant Analysis '
date: 2024-08-08
permalink: /posts/2024/08/LDA/
tags:
  - Machine Learning
  - Classification
---

## Two approaches to classification:
- **Discriminative classifiers** estimate parameters of decision boundary/class
    - Learn $$P(y \mid x)$$ directly (logistic regression models)
    - Learn mappings from inputs to classes (least-squares, neural nets)
- **Generative approach** model the distribution of inputs characteristic of the class (Bayes classifier)
    - Build a model of $$P(x \mid y)$$ or $$P(x,y)$$
    - Apply Bayes Rule
    - Learns the joint probability $$P(x,y)$$ and then transform it to $$P(y \mid x)$$ by using the Bayes rule.

The association between Discriminative and Generative approaches:
\begin{equation}
P(Y=y \mid X=x) = \frac{P(X=x \mid Y=y) P(Y=y)}{P(X=x \mid Y=1)P(Y=1) + P(x \mid Y=0)P(Y=0)}
\end{equation}

**Note:** In statistical classification, the Bayes classifier minimizes the probability of misclassification.

## Statistical Perspective 
Assume that $$X$$ is a $$n \times p$$ matrix. Suppose that $$f_k(x)$$ is the conditional probability that a point with features $x$ is in the class $$Y=k$$; that is, $$P(X=x \mid Y=k)$$. From training data, we can easily estimated the probability $$P(X=x \mid Y=k)$$. However, we are interested in the probability of $$P(Y=k \mid X)$$, which means $$Y$$ can be estimated by given $$X$$. By Baye's theorem, it holds that
\begin{equation}
P(Y=k \mid X) = \frac{P(X=x \mid Y=k) P(Y=k)}{P(X=x)}
\end{equation}

Assume that all of the classes ($$1,..., K$$) have the same covariance matrix ($$\Sigma_i=\Sigma_j, \forall i \neq j$$) and that the classes are distributed as multivariate Gaussian distributions.

Then we may look at the log odds of two classes ($$k$$ and $$l$$), and find that:
\begin{equation}
\log \left\{ \frac{P(Y=k \mid X)}{P(Y=l \mid X)} \right\} &= \log \left\{ \frac{ P(X=x \mid Y=k) P(Y=k)}{P(X=x \mid Y=l) P(Y=l)} \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left\{ \frac{P(X=x \mid Y=k)}{P(X=x \mid Y=l)} \right\}\\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left[ \frac{ \frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \right\} }{\frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\}}  \right] \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) - (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ x\Sigma^{-1}x-2 x^T \Sigma^{-1}\mu_k + \mu_k^T \Sigma^{-1} \mu_k - x\Sigma^{-1}x + 2 x^T \Sigma^{-1}\mu_l - \mu_l^T \Sigma^{-1} \mu_l \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + x^T \Sigma^{-1} (\mu_k-\mu_l) - \frac{1}{2} (\mu_k-\mu_l)^T \Sigma^{-1} (\mu_k-\mu_l)
\end{equation} 

\begin{align}
\log \left\{ \frac{P(Y=k \mid X)}{P(Y=l \mid X)} \right\} &= \log \left\{ \frac{ P(X=x \mid Y=k) P(Y=k)}{P(X=x \mid Y=l) P(Y=l)} \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left\{ \frac{P(X=x \mid Y=k)}{P(X=x \mid Y=l)} \right\}\\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left[ \frac{ \frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \right\} }{\frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\}}  \right] \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) - (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ x\Sigma^{-1}x-2 x^T \Sigma^{-1}\mu_k + \mu_k^T \Sigma^{-1} \mu_k - x\Sigma^{-1}x + 2 x^T \Sigma^{-1}\mu_l - \mu_l^T \Sigma^{-1} \mu_l \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + x^T \Sigma^{-1} (\mu_k-\mu_l) - \frac{1}{2} (\mu_k-\mu_l)^T \Sigma^{-1} (\mu_k-\mu_l)
\end{align} 

$$
\begin{aligned}
\log \left\{ \frac{P(Y=k \mid X)}{P(Y=l \mid X)} \right\} &= \log \left\{ \frac{ P(X=x \mid Y=k) P(Y=k)}{P(X=x \mid Y=l) P(Y=l)} \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left\{ \frac{P(X=x \mid Y=k)}{P(X=x \mid Y=l)} \right\}\\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left[ \frac{ \frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \right\} }{\frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\}}  \right] \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) - (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ x^T \Sigma^{-1}x - 2 x^T \Sigma^{-1}\mu_k + \mu_k^T \Sigma^{-1} \mu_k - x^T \Sigma^{-1}x + 2 x^T \Sigma^{-1}\mu_l - \mu_l^T \Sigma^{-1} \mu_l \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + x^T \Sigma^{-1} (\mu_k-\mu_l) - \frac{1}{2} (\mu_k-\mu_l)^T \Sigma^{-1} (\mu_k-\mu_l)
\end{aligned}
$$

Due to the equal covariance matrix condition, the math simplifies to an equation linear in $$x$$ (the last line). Thus this is a statement that, given multivariate normal distributions with a common covariance matrix, the log-odds between any two classes is a linear function.

