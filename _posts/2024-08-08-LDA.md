---
title: 'Linear Discriminant Analysis '
date: 2024-08-08
permalink: /posts/2024/08/LDA/
tags:
  - Machine Learning
  - Classification
---

## Two Approaches to Classification

- **Discriminative Classifiers**  
  These methods directly estimate the decision boundary or class parameters.  
  - Learn $$P(y \mid x)$$ directly (e.g., logistic regression)
  - Learn mappings from inputs to classes (e.g., least-squares, neural networks)

- **Generative Approach**  
  This method models the distribution of inputs for each class (Bayes classifier).  
  - Build a model of $$P(x \mid y)$$ or $$P(x, y)$$
  - Use Bayes' Rule to convert this into $$P(y \mid x)$$

The relationship between Discriminative and Generative approaches can be expressed as:

\begin{equation}
P(Y=y \mid X=x) = \frac{P(X=x \mid Y=y) P(Y=y)}{P(X=x \mid Y=1)P(Y=1) + P(x \mid Y=0)P(Y=0)}
\end{equation}

**Note:** The Bayes classifier is a powerful tool in statistical classification as it minimizes the probability of misclassification.

## Statistical Perspective

Assume $$X$$ is an $$n \times p$$ matrix. Let's say $$f_k(x)$$ is the conditional probability that a point with features $$x$$ belongs to class $$Y=k$$; that is, $$P(X=x \mid Y=k)$$. From training data, we can estimate $$P(X=x \mid Y=k)$$. However, what we really want is $$P(Y=k \mid X=x)$$, which tells us the probability of class $$Y=k$$ given the features $$X=x$$. By Bayes' theorem:

\begin{equation}
P(Y=k \mid X) = \frac{P(X=x \mid Y=k) P(Y=k)}{P(X=x)}
\end{equation}

Now, assume that all classes ($$1,..., K$$) share the same covariance matrix ($$\Sigma_i=\Sigma_j, \forall i \neq j$$) and that the classes are distributed as multivariate Gaussian distributions.

We can then look at the log odds between two classes ($$k$$ and $$l$$) and find that:

$$
\begin{aligned}
\log \left\{ \frac{P(Y=k \mid X)}{P(Y=l \mid X)} \right\} &= \log \left\{ \frac{ P(X=x \mid Y=k) P(Y=k)}{P(X=x \mid Y=l) P(Y=l)} \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left\{ \frac{P(X=x \mid Y=k)}{P(X=x \mid Y=l)} \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left[ \frac{ \frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \right\} }{\frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\}}  \right] \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} - \frac{1}{2} \left\{ (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) - (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} - \frac{1}{2} \left\{ x^T \Sigma^{-1}x - 2 x^T \Sigma^{-1}\mu_k + \mu_k^T \Sigma^{-1} \mu_k - x^T \Sigma^{-1}x + 2 x^T \Sigma^{-1}\mu_l - \mu_l^T \Sigma^{-1} \mu_l \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + x^T \Sigma^{-1} (\mu_k-\mu_l) - \frac{1}{2} (\mu_k-\mu_l)^T \Sigma^{-1} (\mu_k-\mu_l)
\end{aligned}
$$

Because of the equal covariance matrix assumption, this expression simplifies to a linear function of $$x$$ in the final line. This means that when the distributions are multivariate normal with a common covariance matrix, the log-odds between any two classes is a linear function.

**Note:** This can also be used for supervised **dimensionality reduction**, by projecting the training data onto a linear subspace that maximizes the separation between classes.

## Fisher Linear Discriminant Analysis (LDA)

### Motivation

The main idea here is to find a projection line where samples from different classes are well separated.

Let $$\tilde{\mu}_1$$ and $$\tilde{\mu}_2$$ be the means of the projections of classes 1 ($$C_1$$) and 2 ($$C_2$$). We can measure the separation between the classes by:

$$
\begin{aligned}
\|\tilde{\mu}_2-\tilde{\mu}_1 \|_2^2 
\end{aligned}
$$

This measure should be large for better separation. We also need to normalize this measure by a factor proportional to variance.

We define the scatter as:

$$
\begin{aligned}
\tilde{\sigma}_1^2 = \frac{1}{n_1} \sum_{i \in C_1} \left( w^T x_i - \tilde{\mu}_1 \right)^2
\end{aligned}
$$

Similarly:

$$
\begin{aligned}
\tilde{\sigma}_2^2 = \frac{1}{n_2} \sum_{i \in C_2} \left( w^T x_i - \tilde{\mu}_2 \right)^2.
\end{aligned}
$$

Fisher's Linear Discriminant seeks to project onto a line in the direction $$w$$ that maximizes:

$$
\begin{aligned}
J(w) = \frac{\|\tilde{\mu}_2-\tilde{\mu}_1 \|_2^2 }{\tilde{\sigma}_1^2+\tilde{\sigma}_2^2}
\end{aligned}
$$

The term $$\tilde{\sigma}_1^2$$ indicates that we want the scatter within class 1 to be as small as possible, meaning that samples from class 1 should cluster tightly around their projected mean $$\tilde{\mu}_1$$.

### Derivation

Let's express the scatter within class 1 as:

$$
\begin{aligned}
\tilde{\sigma}_1^2 &= \frac{1}{n_1} \sum_{i \in C_1} \left( w^T x_i - \tilde{\mu}_1 \right)^2 = \frac{1}{n_1} \sum_{i \in C_1} \left( w^T x_i - w^T \mu_1 \right)^2 \\
&= w^T \left\{ \frac{1}{n_1} \sum_{i \in C_1} (x_i-\mu_1)^2 \right\} w \\
&= w^T S_1 w
\end{aligned}
$$

So, the total scatter for both classes is:

$$
\begin{aligned}
\tilde{\sigma}_1^2+\tilde{\sigma}_2^2 = w^T S_1 w + w^T S_2 w = w^T S_W w
\end{aligned}
$$

where $$S_W=S_1+S_2$$ is called the within-class scatter matrix.

Now, consider the between-class scatter:

$$
\begin{aligned}
\|\tilde{\mu}_2-\tilde{\mu}_1\|_2^2 = w^T (\mu_2-\mu_1)^T (\mu_2-\mu_1) w = w^T S_B w
\end{aligned}
$$

where $$S_B$$ is called the between-class scatter matrix. Our objective function can then be written as:

$$
\begin{aligned}
J(w) = \frac{\|\tilde{\mu}_2-\tilde{\mu}_1 \|_2^2 }{\tilde{\sigma}_1^2+\tilde{\sigma}_2^2} = \frac{w^T S_B w}{w^T S_W w}
\end{aligned}
$$

To minimize $$J(w)$$, we take the derivative with respect to $$w$$ and set it to zero:

$$
\begin{aligned}
\frac{d}{dw}J(w) = \frac{(2S_Bw)w^T S_W w - (2S_W w)w^T S_B w }{(w^T S_W w)^2} = 0
\end{aligned}
$$

This simplifies to:

$$
\begin{aligned}
&w^T S_W w (S_B w) - w^T S_B w (S_W w) = 0 \\
\Rightarrow &\frac{w^T S_W w (S_B w)}{w^T S_W w} - \frac{w^T S_B w (S_W w)}{w^T S_W w} = 0 \\
\Rightarrow &S_B w - \frac{w^T S_B w}{w^T S_W w} (S_W)w = 0 \\
\Rightarrow &S_B w = \lambda S_W w
\end{aligned}
$$

This optimization problem can be transformed into a generalized eigenvalue problem.

If $$S_W$$ is full rank (i.e., it has an inverse), we can convert this into a standard eigenvalue problem:

$$
\begin{aligned}
S_W^{-1}S_B w = \lambda w
\end{aligned}
$$

However, for any vector $$x$$, $$S_B x$$ points in the same direction as $$\mu_1-\mu_2$$:

$$
\begin{aligned}
S_B x = (\mu_1-\mu_2)(\mu_1-\mu_2)^T x = (\mu_1-\mu_2) \alpha = \alpha (\mu_1-\mu_2)
\end{aligned}
$$

where $$\alpha$$ is a constant. This allows us to solve the eigenvalue problem immediately:

$$
\begin{aligned}
w = S_W^{-1}(\mu_1-\mu_2)
\end{aligned}
$$

This implies that:

$$
\begin{aligned}
S_W^{-1}S_B \{ S_W^{-1} (\mu_1-\mu_2) \} = S_W^{-1} \alpha (\mu_1-\mu_2)
\end{aligned}
$$

where $$\alpha$$ is an eigenvalue, and $$w=S_W^{-1}(\mu_1-\mu_2)$$ is the corresponding eigenvector.

### Relationship Between Statistical Perspective and Fisher LDA

$$
\begin{aligned}
\hat{\Sigma} &= \left( \frac{n_1}{n} \right) \frac{1}{n_1} \sum_{i \in C_1} (x_i-\mu_1)(x_i-\mu_1)^T + \left( \frac{n_2}{n} \right) \frac{1}{n_2} \sum_{i \in C_2} (x_i-\mu_2)(x_i-\mu_2)^T \\ 
&= \frac{1}{n} (S_1+S_2) = \frac{1}{n} S_W
\end{aligned}
$$

Thus,

$$
\begin{aligned}
w \propto n S_W^{-1} (\mu_1-\mu_2)
\end{aligned}
$$

**Although the approaches are different, the result is surprisingly similar.**

**Generative algorithms** make specific structural assumptions about the model, whereas **Discriminative algorithms** make fewer assumptions. For instance, Naive Bayes assumes conditional independence of features, while logistic regression (the discriminative counterpart to Naive Bayes) does not.

