---
title: 'Linear Discriminant Analysis '
date: 2024-08-08
permalink: /posts/2024/08/LDA/
tags:
  - Machine Learning
  - Classification
---

## Two Approaches to Classification

- **Discriminative Classifiers**  
  These methods estimate the parameters that define the decision boundaries between classes.  
  - Directly learn $$P(y \mid x)$$ (e.g., logistic regression)
  - Learn mappings from inputs to classes (e.g., least-squares, neural networks)

- **Generative Classifiers**  
  These methods model the distribution of inputs for each class and apply Bayes' Rule.  
  - Build a model of $$P(x \mid y)$$ or $$P(x, y)$$
  - Apply Bayes' Rule to convert this into $$P(y \mid x)$$

**Relationship Between Discriminative and Generative Approaches:**

$$
\begin{aligned}
P(Y = y \mid X = x) = \frac{P(X = x \mid Y = y) \cdot P(Y = y)}{P(X = x \mid Y = 1) \cdot P(Y = 1) + P(X = x \mid Y = 0) \cdot P(Y = 0)}
\end{aligned}
$$

**Note:** The Bayes classifier minimizes the probability of misclassification, making it a powerful tool in statistical classification.

## Statistical Perspective

Let's assume $$X$$ is an $$n \times p$$ matrix. Suppose $$f_k(x)$$ represents the conditional probability that a point with features $$x$$ belongs to class $$Y = k$$, which is denoted as $$P(X = x \mid Y = k)$$. From training data, we can estimate this probability. However, we are more interested in $$P(Y = k \mid X = x)$$, the probability of the class given the features. By Bayes' theorem:

$$
\begin{aligned}
P(Y = k \mid X = x) = \frac{P(X = x \mid Y = k) \cdot P(Y = k)}{P(X = x)}
\end{aligned}
$$

If we assume that all classes share the same covariance matrix ($$\Sigma_i = \Sigma_j, \forall i \neq j$$) and are normally distributed, we can simplify the log-odds between two classes:

$$
\begin{aligned}
\log \left\{ \frac{P(Y = k \mid X)}{P(Y = l \mid X)} \right\} &= \log \left\{ \frac{P(Y = k)}{P(Y = l)} \right\} + x^T \Sigma^{-1} (\mu_k - \mu_l) \\
&\quad - \frac{1}{2} (\mu_k - \mu_l)^T \Sigma^{-1} (\mu_k - \mu_l)
\end{aligned}
$$

This equation shows that, under these assumptions, the log-odds between any two classes is a linear function of $$x$$.

**Note:** This linear relationship can also be used for supervised **dimensionality reduction**, projecting data onto directions that maximize the separation between classes.

## Fisher Linear Discriminant Analysis (LDA)

### Motivation

The main idea is to find a projection line where samples from different classes are well-separated.

Let $$\tilde{\mu}_1$$ and $$\tilde{\mu}_2$$ be the means of the projections of classes 1 ($$C_1$$) and 2 ($$C_2$$):

$$
\begin{aligned}
\|\tilde{\mu}_2 - \tilde{\mu}_1 \|_2^2
\end{aligned}
$$

This value is a good measure of separation. The larger it is, the better the separation. We also normalize this by a factor proportional to variance.

Define the scatter as:

$$
\begin{aligned}
\tilde{\sigma}_1^2 &= \frac{1}{n_1} \sum_{i \in C_1} \left( w^T x_i - \tilde{\mu}_1 \right)^2
\end{aligned}
$$

Similarly,

$$
\begin{aligned}
\tilde{\sigma}_2^2 &= \frac{1}{n_2} \sum_{i \in C_2} \left( w^T x_i - \tilde{\mu}_2 \right)^2
\end{aligned}
$$

Fisher's Linear Discriminant maximizes the ratio:

$$
\begin{aligned}
J(w) = \frac{\|\tilde{\mu}_2 - \tilde{\mu}_1 \|_2^2 }{\tilde{\sigma}_1^2 + \tilde{\sigma}_2^2}
\end{aligned}
$$

This ensures that the scatter within each class is minimized, meaning the samples cluster closely around their projected means.

### Derivation

We start by expressing the scatter within a class:

$$
\begin{aligned}
\tilde{\sigma}_1^2 = w^T S_1 w
\end{aligned}
$$

So,

$$
\begin{aligned}
\tilde{\sigma}_1^2 + \tilde{\sigma}_2^2 = w^T S_W w
\end{aligned}
$$

where $$S_W = S_1 + S_2$$ is the within-class scatter matrix.

On the other hand, the between-class scatter is:

$$
\begin{aligned}
\|\tilde{\mu}_2 - \tilde{\mu}_1\|_2^2 = w^T S_B w
\end{aligned}
$$

where $$S_B$$ is the between-class scatter matrix. Thus, the objective function is:

$$
\begin{aligned}
J(w) = \frac{w^T S_B w}{w^T S_W w}
\end{aligned}
$$

We optimize this by taking the derivative with respect to $$w$$ and setting it to zero:

$$
\begin{aligned}
\frac{d}{dw}J(w) = \frac{(2S_B w) w^T S_W w - (2S_W w) w^T S_B w }{(w^T S_W w)^2} = 0
\end{aligned}
$$

This leads to:

$$
\begin{aligned}
S_B w = \lambda S_W w
\end{aligned}
$$

This optimization problem is equivalent to solving a generalized eigenvalue problem. If $$S_W$$ has full rank, we can convert this to a standard eigenvalue problem:

$$
\begin{aligned}
S_W^{-1}S_B w = \lambda w
\end{aligned}
$$

Finally, we find that the solution is:

$$
\begin{aligned}
w = S_W^{-1}(\mu_1 - \mu_2)
\end{aligned}
$$

This implies that $$w$$ is the eigenvector associated with the largest eigenvalue, representing the direction that maximizes class separation.

## Relationship between Statistical Perspective and Fisher LDA

$$
\begin{aligned}
\hat{\Sigma} &= \left( \frac{n_1}{n} \right) \frac{1}{n_1} \sum_{i \in C_1} (x_i-\mu_1)(x_i-\mu_1)^T + \left( \frac{n_2}{n} \right) \frac{1}{n_2} \sum_{i \in C_2} (x_i-\mu_2)(x_i-\mu_2)^T \\ 
&= \frac{1}{n} (S_1+S_2) = \frac{1}{n} S_W
\end{aligned}
$$

Thus,

$$
\begin{aligned}
w \propto n S_W^{-1} (\mu_1-\mu_2)
\end{aligned}
$$

**Although the philosophy between them is different, the result is surprisingly equivalent.**

**Generative algorithms** make some kind of structural assumptions on your model, but **Discriminative algorithms** make fewer assumptions. For example, Naive Bayes assumes conditional independence of your features, while logistic regression (the discriminative "counterpart" of Naive Bayes) does not.



