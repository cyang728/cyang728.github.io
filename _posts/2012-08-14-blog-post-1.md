---
title: 'Linear Discriminant Analysis '
date: 2024-08-08
permalink: /posts/2024/08/LDA/
tags:
  - Machine Learning
  - Classification
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool.

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------

---
tags: 機器學習 (Machine Learning)
---

# Linear Discriminant Analysis 

## Two approaches to classification:
- **Discriminative classifiers** estimate parameters of decision boundary / class
    - Learn $P(y \mid x)$ directly (logistic regression models)
    - Learn mappings from inputs to classes (least-squares, neural nets)
- **Generative approach** model the distribution of inputs characteristic of the class (Bayes classifier)
    - Build a model of $P(x \mid y)$ or $P(x,y)$
    - Apply Bayes Rule
    - Learns the joint probability $P(x,y)$ and then transforms it to $P(y \mid x)$ by using Bayes' rule.

The association between Discriminative and Generative approaches:
\[
P(Y=y \mid X=x) = \frac{P(X=x \mid Y=y) P(Y=y)}{P(X=x \mid Y=1)P(Y=1) + P(x \mid Y=0)P(Y=0)}
\]
:::info
In statistical classification, the Bayes classifier minimizes the probability of misclassification.
- Naive Bayes Classifier
:::

## Statistical Perspective 
Assume that $X$ is a $n \times p$ matrix. Suppose that $f_k(x)$ is the conditional probability that a point with features $x$ is in the class $Y=k$; that is, $P(X=x \mid Y=k)$. From training data, we can easily estimate the probability $P(X=x \mid Y=k)$. However, we are interested in the probability of $P(Y=k \mid X)$, which means $Y$ can be estimated by given $X$. By Bayes' theorem, it holds that
\[
P(Y=k \mid X) = \frac{P(X=x \mid Y=k) P(Y=k)}{P(X=x)}
\]

Assume that all of the classes ($1,\dots,K$) have the same covariance matrix ($\Sigma_i=\Sigma_j, \forall i \neq j$) and that the classes are distributed as multivariate Gaussian distributions.

Then we may look at the log-odds of two classes ($k$ and $l$), and find that:
\[
\begin{aligned}
\log \left\{ \frac{P(Y=k \mid X)}{P(Y=l \mid X)} \right\} &= \log \left\{ \frac{ P(X=x \mid Y=k) P(Y=k)}{P(X=x \mid Y=l) P(Y=l)} \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left\{ \frac{P(X=x \mid Y=k)}{P(X=x \mid Y=l)} \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + \log \left[ \frac{ \frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \right\} }{\frac{1}{\sqrt{2 \pi }^{p}|\Sigma|} \exp \left\{ -\frac{1}{2} (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\}}  \right] \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) - (x-\mu_l)^T \Sigma^{-1} (x-\mu_l) \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} -
\frac{1}{2} \left\{ x\Sigma^{-1}x-2 x^T \Sigma^{-1}\mu_k + \mu_k^T \Sigma^{-1} \mu_k - x\Sigma^{-1}x + 2 x^T \Sigma^{-1}\mu_l - \mu_l^T \Sigma^{-1} \mu_l \right\} \\
&= \log \left\{ \frac{P(Y=k)}{P(Y=l)} \right\} + x^T \Sigma^{-1} (\mu_k-\mu_l) - \frac{1}{2} (\mu_k-\mu_l)^T \Sigma^{-1} (\mu_k-\mu_l)
\end{aligned}
\]
Due to the equal covariance matrix condition, the math simplifies to an equation linear in $x$ (the last line). Thus this is a statement that, given multivariate normal distributions with a common covariance matrix, the log-odds between any two classes is a linear function.

:::success
### Normality assumptions:
> "linear discriminant analysis frequently achieves good performances in the tasks of face and object recognition, even though the assumptions of common covariance matrix among groups and normality are often violated (Duda, et al., 2001)” (Tao Li, et al., 2006).
>> Tao Li, Shenghuo Zhu, and Mitsunori Ogihara. “Using Discriminant Analysis for Multi-Class Classification: An Experimental Investigation.” Knowledge and Information Systems 10, no. 4 (2006): 453–72.
>> Duda, Richard O, Peter E Hart, and David G Stork. 2001. Pattern Classification. New York: Wiley.
:::

:::info
### Dimensionality reduction:
It can be used to perform supervised dimensionality reduction by projecting the training data to a linear subspace consisting of the directions that maximize the separation between classes.
:::

## Fisher Linear Discriminant Analysis
### Motivation 
:::success
Main idea: find projection to a line such that samples from different classes can be well separated.
:::
Let $\tilde{\mu}_1$ and $\tilde{\mu}_2$ be the means of projections of classes 1 ($C_1$) and 2 ($C_2$). 
\[
\|\tilde{\mu}_2-\tilde{\mu}_1 \|_2^2 
\]
seems like a good measure.
\[
\begin{aligned}
\tilde{\mu}_1 &= \frac{1}{n_1} \sum_{i=1}^n w^T x_i = w^T \left( \frac{1}{n_1} \sum_{i \in C_1} x_i \right) = w^T \mu_1  \\
\tilde{\mu}_2 &= w^T \mu_2
\end{aligned}
\]

The larger $\|\tilde{\mu}_2-\tilde{\mu}_1\|_2^2$, the better the expected separation. Moreover, we need to normalize $\|\tilde{\mu}_2-\tilde{\mu}_1\|_2^2$ by a factor proportional to variance.

:::success
Define their scatter as:
\[
\tilde{\sigma}_1^2 = \frac{1}{n_1} \sum_{i \in C_1} \left( w^T x_i - \tilde{\mu}_1 \right)^2
\]
Similarly,
\[
\tilde{\sigma}_2^2 = \frac{1}{n_1} \sum_{i \in C_2} \left( w^T x_i - \tilde{\mu}_2 \right)^2
\]
:::
Thus Fisher linear discriminant is to project on a line in the direction $w$ which maximizes:
\[
J(w) = \frac{\|\tilde{\mu}_2-\tilde{\mu}_1 \|_2^2 }{\tilde{\sigma}_1^2+\tilde{\sigma}_2^2}
\]
:::info
$\tilde{\sigma}_1^2$ means that we want the scatter in class 1 to be as small as possible, i.e., samples of class 1 cluster around the projected mean $\tilde{\mu}_1$.
:::
### Derivation 
Let 
\[
\begin{aligned}
\tilde{\sigma}_1^2 &= \frac{1}{n_1} \sum_{i \in C_1} \left( w^T x_i - \tilde{\mu}_1 \right)^2 = \frac{1}{n_1} \sum_{i \in C_1} \left( w^T x_i - w^T \mu_1 \right)^2 \\
&= w^T \left\{ \frac{1}{n_1} \sum_{i \in C_1} (x_i-\mu_1)^2 \right\} w \\
&= w^T S_1 w
\end{aligned}
\]
Therefore,
\[
\tilde{\sigma}_1^2+\tilde{\sigma}_2^2 = w^T S_1 w + w^T S_2 w = w^T S_W w
\]
where $S_W=S_1+S_2$ is defined as the within-class scatter matrix.

On the other hand,
\[
\|\tilde{\mu}_2-\tilde{\mu}_1\|_2^2 = w^T (\mu_2-\mu_1)^T (\mu_2-\mu_1) w = w^T S_B w
\]
where $S_B$ is defined as the between-class scatter matrix. Thus our objective function can be written as:
\[
J(w) = \frac{\|\tilde{\mu}_2-\tilde{\mu}_1 \|_2^2 }{\tilde{\sigma}_1^2+\tilde{\sigma}_2^2} = \frac{w^T S_B w}{w^T S_W w}
\]
Minimize $J(w)$ by taking the derivative with respect to $w$ and setting it to $0$:
\[
\frac{d}{dw}J(w) = \frac{(2S_Bw)w^T S_W w - (2S_W w)w^T S_B w }{(w^T S_W w)^2} = 0
\]
Therefore,
\[
\begin{aligned}
&w^T S_W w (S_B w) - w^T S_B w (S_W w) = 0 \\
\Rightarrow &\frac{w^T S_W w (S_B w)}{w^T S_W w} - \frac{w^T S_B w (S_W w)}{w^T S_W w} = 0 \\
\Rightarrow &S_B w - \frac{w^T S_B w}{w^T S_W w} (S_W)w = 0 \\
\Rightarrow &S_B w = \lambda S_W w
\end{aligned}
\]
Thus, the optimization can be transformed into a generalized eigenvalue problem.

If $S_W$ has full rank (the inverse exists), we can convert this to a standard eigenvalue problem:
\[
S_W^{-1}S_B w = \lambda w
\]
However, $S_B x$ for any vector $x$ points in the same direction as $\mu_1-\mu_2$, i.e.,
\[
S_B x = (\mu_1-\mu_2)(\mu_1-\mu_2)^T x = (\mu_1-\mu_2) \alpha = \alpha (\mu_1-\mu_2)
\]
where $\alpha$ becomes a constant. Thus we can solve the eigenvalue problem immediately:
\[
w = S_W^{-1}(\mu_1-\mu_2)
\]
It implies that 
\[
S_W^{-1}S_B \{ S_W^{-1} (\mu_1-\mu_2) \} = S_W^{-1} \alpha (\mu_1-\mu_2)
\]
where $\alpha$ can be seen as an eigenvalue, and $w=S_W^{-1}(\mu_1-\mu_2)$ is the eigenvector corresponding to the $\alpha$.

:::warning
### Relationship between statistical perspective and Fisher LDA
\[
\begin{aligned}
\hat{\Sigma} &= \left( \frac{n_1}{n} \right) \frac{1}{n_1} \sum_{i \in C_1} (x_i-\mu_1)(x_i-\mu_1)^T + \left( \frac{n_2}{n} \right) \frac{1}{n_2} \sum_{i \in C_2} (x_i-\mu_2)(x_i-\mu_2)^T \\ 
&= \frac{1}{n} (S_1+S_2) = \frac{1}{n} S_W
\end{aligned}
\]
Thus,
\[
w \propto n S_W^{-1} (\mu_1-\mu_2)
\]
**Although the philosophy between them is different, the result is surprisingly equivalent.**
:::

:::info
**Generative algorithms** make some kind of structural assumptions on your model, but **Discriminative algorithms** make fewer assumptions. For example, Naive Bayes assumes conditional independence of your features, while logistic regression (the discriminative "counterpart" of Naive Bayes) does not.
:::

## Application 
### Face verification
:::info
According to the papers:
- **Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection**
- **Bayesian Face Revisited: A Joint Formulation** 
- **Deep Learning Face Representation from Predicting 10,000 Classes**
:::
Let $x_1$ and $x_2$ be two images. Our goal is to identify whether $x_1$ and $x_2$ are from the same person or not. 

Joint Bayesian technique for face verification based on the DeepID:
\[
x = \mu+\epsilon
\]
$\mu\sim N(0,S_\mu)$: the face identity.
$\epsilon\sim N(0,S_\epsilon)$: intra-personal variations.
and
\[
\begin{aligned}
\begin{bmatrix}
x_1 \\ x_2
\end{bmatrix}
\begin{bmatrix}
x_1^T & x_2^T
\end{bmatrix} &=
\begin{bmatrix}
\mu_1+\epsilon_1 \\ \mu_2+\epsilon_2
\end{bmatrix}
\begin{bmatrix}
(\mu_1+\epsilon_1)^T & (\mu_2+\epsilon_2)^T
\end{bmatrix} \\
&=\begin{bmatrix}
(\mu_1+\epsilon_1)(\mu_1+\epsilon_1)^T & (\mu_1+\epsilon_1)(\mu_2+\epsilon_2)^T \\
(\mu_2+\epsilon_2)(\mu_1+\epsilon_1)^T & (\mu_2+\epsilon_2)(\mu_2+\epsilon_2)^T
\end{bmatrix} \\
&= \begin{bmatrix}
\mu_1 \mu_1^T + \mu_1 \epsilon_1^T + \epsilon_1 \mu_1^T + \epsilon_1 \epsilon_1^T & \mu_1 \mu_2^T + \mu_1 \epsilon_2^T + \epsilon_1 \mu_2^T + \epsilon_1 \epsilon_2^T \\
\mu_2 \mu_1^T + \mu_2 \epsilon_1^T + \epsilon_2 \mu_1^T + \epsilon_2 \epsilon_1^T & \mu_2 \mu_2^T + \mu_2 \epsilon_2^T + \epsilon_2 \mu_2^T + \epsilon_2 \epsilon_2^T 
\end{bmatrix}
\end{aligned}
\]
Hence, $P(x_1,x_2 \mid H_I)$ can be viewed as a joint probability of two pictures under intra-personal (same) variation hypothesis.
\[
\Sigma_{I} = 
\begin{bmatrix}
S_{\mu}+S_{\epsilon} & S_{\mu} \\
S_{\mu}              & S_{\mu}+S_{\epsilon}
\end{bmatrix}
\]
On the other hand, both the identities and intra-personal variations are independent under $H_E$. Hence, the covariance matrix of the distribution $P (x_1, x_2 \mid H_E)$ is:
\[
\Sigma_{E} = 
\begin{bmatrix}
S_{\mu}+S_{\epsilon} & 0 \\
0                    & S_{\mu}+S_{\epsilon}
\end{bmatrix}
\]
With the above two conditional joint probabilities, the log-likelihood ratio $r(x_1, x_2)$ can be obtained in a closed form after simple algebra operations:
\[
r(x_1, x_2) = \log \frac{P (x_1, x_2 \mid H_I)}{P (x_1, x_2 \mid H_E)}
\]
which has closed-form solutions and is efficient.

#### References:
- https://ccjou.wordpress.com/2014/03/14/%E8%B2%BB%E9%9B%AA%E7%9A%84%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90%E8%88%87%E7%B7%9A%E6%80%A7%E5%88%A4%E5%88%A5%E5%88%86%E6%9E%90/
- https://towardsdatascience.com/an-illustrative-introduction-to-fishers-linear-discriminant-9484efee15ac
